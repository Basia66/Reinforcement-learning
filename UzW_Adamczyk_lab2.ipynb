{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Łańcuch Markowa (Markov chain)\n",
    "\n",
    "Łańcuchy Markowa modelują procesy stochastyczne, które ewoluują w czasie zgodnie z wewnętrzną dynamiką.\n",
    "**Stan** jest to kompletny zbiór informacji, który opisuje sytuację, w jakiej znalazło się dane środowisko. Jeżeli *tranzycja* do kolejnego stanu, zależy jedynie od stanu aktualnego (nie rozpatrujemy pod uwagę tych stanów, w których środowisko już było wcześniej), mówimy, że proces posiada **własność Markowa**.\n",
    "![](img/markov_property.png)\n",
    "![](img/robot_markov_chain.png)\n",
    "Powyższy obrazek przedstawia zepsutego robota, który losowo porusza się po środowisku. Robot porusza się w zadanym kierunku zgodnie z prawdopodobieństwem określonym na rysunku. Będąc w stanie (1,2), nie ma znaczenia, jaką drogę przebył, aby się tam znaleźć. Przejście do kolejnego stanu zależy jedynie od aktualnego (własność Markowa).\n",
    "![](img/absorbing.png)\n",
    "Załóżmy, że jeżeli robot uderzy w ścianę (tj. poruszy się w jej kierunku) rozbija się i nie może się już poruszyć. Taki stan nazywamy *terminalnym*, który kończy *epizod*.\n",
    "\n",
    "## Proces nagród Markowa (Markov reward process)\n",
    "\n",
    "Każdy system posiada stany, w których lepiej byłoby się znajdować i takie, w których mniej. Łańcuch Markowa nie mówi nam nic na temat przewagi jednych stanów nad drugimi. Do tego potrzebujemy **nagród** przypisanych do poszczególnych stanów procesu.\n",
    "\n",
    "### Przykład\n",
    "\n",
    "Rozpatrzmy przykładowe środowisko typu *grid world* o wielkości 3x3 (stany indeksowane są następująco: (0,0):1, (0,1):2, ... , (2,2):9) ze stanem terminalnym, do którego agent przechodzi, gdy uderzy w ścianę."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "m = 3\n",
    "m2 = m ** 2  # liczba stanów\n",
    "q = np.zeros(m2)\n",
    "q[m2 // 2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: (0, 0),\n",
       " 2: (0, 1),\n",
       " 3: (0, 2),\n",
       " 4: (1, 0),\n",
       " 5: (1, 1),\n",
       " 6: (1, 2),\n",
       " 7: (2, 0),\n",
       " 8: (2, 1),\n",
       " 9: (2, 2)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix_map = {i + 1: (i // m, i % m) for i in range(m2)}\n",
    "ix_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# funkcja tworząca macierz przejść z określonymi prawdopodobieństwami tranzycji pomiędzy stanami\n",
    "def get_P(m, p_up, p_down, p_left, p_right):\n",
    "    m2 = m ** 2\n",
    "    P = np.zeros((m2, m2))\n",
    "    ix_map = {i + 1: (i // m, i % m) for i in range(m2)}\n",
    "    for i in range(m2):\n",
    "        for j in range(m2):\n",
    "            r1, c1 = ix_map[i + 1]\n",
    "            r2, c2 = ix_map[j + 1]\n",
    "            rdiff = r1 - r2\n",
    "            cdiff = c1 - c2\n",
    "            if rdiff == 0:\n",
    "                if cdiff == 1:\n",
    "                    P[i, j] = p_left\n",
    "                elif cdiff == -1:\n",
    "                    P[i, j] = p_right\n",
    "                elif cdiff == 0:\n",
    "                    if r1 == 0:\n",
    "                        P[i, j] += p_down\n",
    "                    elif r1 == m - 1:\n",
    "                        P[i, j] += p_up\n",
    "                    if c1 == 0:\n",
    "                        P[i, j] += p_left\n",
    "                    elif c1 == m - 1:\n",
    "                        P[i, j] += p_right\n",
    "            elif rdiff == 1:\n",
    "                if cdiff == 0:\n",
    "                    P[i, j] = p_down\n",
    "            elif rdiff == -1:\n",
    "                if cdiff == 0:\n",
    "                    P[i, j] = p_up\n",
    "    return P\n",
    "\n",
    "# Poszerzenie macierzy P o stan terminalny 'crashed'\n",
    "P = np.zeros((m2 + 1, m2 + 1))\n",
    "P[:m2, :m2] = get_P(3, 0.2, 0.3, 0.25, 0.25)\n",
    "for i in range(m2):\n",
    "    P[i, m2] = P[i, i]\n",
    "    P[i, i] = 0\n",
    "P[m2, m2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.55, 0.25, 0.  , 0.2 , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.25, 0.3 , 0.25, 0.  , 0.2 , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.25, 0.55, 0.  , 0.  , 0.2 , 0.  , 0.  , 0.  ],\n",
       "       [0.3 , 0.  , 0.  , 0.25, 0.25, 0.  , 0.2 , 0.  , 0.  ],\n",
       "       [0.  , 0.3 , 0.  , 0.25, 0.  , 0.25, 0.  , 0.2 , 0.  ],\n",
       "       [0.  , 0.  , 0.3 , 0.  , 0.25, 0.25, 0.  , 0.  , 0.2 ],\n",
       "       [0.  , 0.  , 0.  , 0.3 , 0.  , 0.  , 0.45, 0.25, 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.3 , 0.  , 0.25, 0.2 , 0.25],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.3 , 0.  , 0.25, 0.45]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P2 = get_P(3, 0.2, 0.3, 0.25, 0.25)\n",
    "P2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.25, 0.  , 0.2 , 0.  , 0.  , 0.  , 0.  , 0.  , 0.55],\n",
       "       [0.25, 0.  , 0.25, 0.  , 0.2 , 0.  , 0.  , 0.  , 0.  , 0.3 ],\n",
       "       [0.  , 0.25, 0.  , 0.  , 0.  , 0.2 , 0.  , 0.  , 0.  , 0.55],\n",
       "       [0.3 , 0.  , 0.  , 0.  , 0.25, 0.  , 0.2 , 0.  , 0.  , 0.25],\n",
       "       [0.  , 0.3 , 0.  , 0.25, 0.  , 0.25, 0.  , 0.2 , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.3 , 0.  , 0.25, 0.  , 0.  , 0.  , 0.2 , 0.25],\n",
       "       [0.  , 0.  , 0.  , 0.3 , 0.  , 0.  , 0.  , 0.25, 0.  , 0.45],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.3 , 0.  , 0.25, 0.  , 0.25, 0.2 ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.3 , 0.  , 0.25, 0.  , 0.45],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# końcowa macierz tranzycji, ze stanu 9 agent nie może już przejść do żadnego innego poza stanem 9.\n",
    "# Ze stanu pierwszego istnieje 0.55 szansy, że agent uderzy w ścianę, 0.25, że przejdzie w prawo (stan 2) oraz\n",
    "# 0.2 szansy że pójdzie do góry (stan 4)\n",
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Agent otrzymuje nagrodę 1 za każdy ruch. Gdy wpadnie w ścianę, epizod się kończy, a agent otrzymuje nagrodę równą 0.\n",
    "Agent rozpoczyna epizod w stanie początkowym, a następne zdobyte przez agenta nagrody są zapisywane.\n",
    "Pozwoli to później na określenie średniej (oczekiwanej) wartości **sumy nagród** (*return*, suma nagród w danym epizodzie) dla każdego ze stanów środowiska.\n",
    "Taką wartość nazywamy **wartością stanu (state value)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n = 10 ** 5\n",
    "avg_rewards = np.zeros(m2)\n",
    "for s in range(9): # dla każdego stanu początkowego\n",
    "    for i in range(n): # symulacja n epizodów\n",
    "        crashed = False\n",
    "        s_next = s\n",
    "        episode_reward = 0\n",
    "        while not crashed: # symulacja przejść, dopóki agent nie wpadnie w ścianę\n",
    "            s_next = np.random.choice(m2 + 1, p=P[s_next, :])\n",
    "            if s_next < m2: # wewnątrz ścian\n",
    "                episode_reward += 1\n",
    "            else: # zderzenie ze ścianą\n",
    "                crashed = True\n",
    "        avg_rewards[s] += episode_reward\n",
    "avg_rewards /= n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.47, 2.1 , 1.48, 2.43, 3.43, 2.45, 1.99, 2.82, 2.  ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(avg_rewards, 2) # średnie sumy nagród"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Zadanie #1\n",
    "\n",
    "Który stan jest najlepszy, a który najgorszy i dlaczego? Z czego wynika przewaga jednego nad drugim?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najlepszy stan: 5. Największa suma nagród (3.41). Najczęściej odwiedzany przez agenta. <br>\n",
    "Najgorszy stan: 1 lub 3. Najmniejsza suma nagród (1.47 lub 1.48). Najrzadziej odwiedzane przez agenta. <br>\n",
    "\n",
    "Dodatkowo stan 5 ma 4 sąsiadów, co oznacza, że szansa na odwiedzenie tego stanu przez agenta jest największa oraz w tym stanie agent nie może uderzyć w ścianę (p=0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie #2\n",
    "\n",
    "Wartość stanu definiowana jest jako oczekiwana zdyskontowana suma nagród, gdy zaczynamy ze stanu *s*:\n",
    "![](img/sv_mrp.png)\n",
    "Co oznacza **współczynnik dyskontowania γ**? Jak go możemy interpretować?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Współczynnik dyskontowania reguluje względną ważność krótko i długoterminowych nagród. <br>\n",
    "$$\n",
    "{\\gamma \\in [0,1]\\,}\n",
    "$$\n",
    "gdzie gdy jest blisko 0, to oznacza, że bierzemy pod uwagę krótkoterminowe nagrody, a jak blisko 1, to długoterminowe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Rekurencyjna relacja pomiędzy wartościami stanów\n",
    "\n",
    "Możemy wyznaczyć wartość stanu ze stanów sąsiadujących (takich, do których agent może przejść ze stanu rozpatrywanego) zgodnie z formułą rekurencyjną:\n",
    "![](img/recursive_relationship_formula.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4205"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# estymacja wartości (state value) czwartego stanu z wartości stanów sąsiadujących\n",
    "(1 + 2.45) * 0.25 + (1 + 2.44) * 0.25 + 0.2 * (1+2.81) + 0.3*(1+2.12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Powyższą formułę nazywamy **równaniem Bellmana dla MRP** i możemy ją zapisać następująco (uwzględniając już wsp. dyskontowania):\n",
    "![](img/bellman_eq_mrp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Zadanie #3\n",
    "\n",
    "Wyznacz używając reguły rekurencyjnej wartości stanów dla pozostałych stanów środowiska (v(0,0), v(1,0), ..., v(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v(0,0) = 1.461\n",
      "v(1,0) = 2.1235\n",
      "v(2,0) = 1.465\n",
      "v(0,1) = 2.4465\n",
      "v(1,1) = 3.4140000000000006\n",
      "v(2,1) = 2.4515000000000002\n",
      "v(0,2) = 1.984\n",
      "v(1,2) = 2.8265000000000002\n",
      "v(2,2) = 1.9899999999999998\n"
     ]
    }
   ],
   "source": [
    "states = [(0,0), (1,0), (2,0), (0,1), (1,1), (2,1), (0,2), (1,2), (2,2)]\n",
    "a = len(states)      \n",
    "v = np.round(avg_rewards, 2)\n",
    "for i in range(a):\n",
    "    value = 0\n",
    "    for j in range(a):\n",
    "        value = value + P[i][j]*(1+v[j])\n",
    "    print(\"v(\" + str(states[i][0]) + \",\" + str(states[i][1]) + \") = \" + str(value));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Iteracyjna estymacja wartości stanów\n",
    "\n",
    "Jedną z centralnych idei UzW jest iteracyjna możliwość estymacji wartości stanów.\n",
    "Użyjmy równania Bellmana jako reguły aktualizacji wartości.\n",
    "Estymację kończymy, gdy aktualizacja dla wartości stanu jest mniejsza niż zadana wartość progowa (threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def estimate_state_values(P, m2, threshold):\n",
    "    v = np.zeros(m2 + 1)\n",
    "    max_change = threshold\n",
    "    terminal_state = m2\n",
    "    while max_change >= threshold:\n",
    "        max_change = 0\n",
    "        for s in range(m2 + 1): # dla każdego stanu\n",
    "            v_new = 0\n",
    "            for s_next in range(m2 + 1): # aktualizacja wartości stanów\n",
    "                r = 1 * (s_next != terminal_state)\n",
    "                v_new += P[s, s_next] * (r + v[s_next])\n",
    "            max_change = max(max_change, np.abs(v[s] - v_new))\n",
    "            v[s] = v_new\n",
    "    return np.round(v, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.47, 2.12, 1.47, 2.44, 3.42, 2.44, 1.99, 2.82, 1.99, 0.  ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_state_values(P, m2, 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Zadanie #4\n",
    "Dlaczego to podejście jest niepraktyczne w praktyce? Podaj dwa powody.\n",
    "\n",
    "*Tip: ma to związek z przestrzenią stanów oraz macierzą P.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) duża złożoność obliczeniowa funkcji <br>\n",
    "Mamy pętle while i dwie pętle for, przy dużym środowisku, algorytm może wykonywać się bardzo długo. <br>\n",
    "\n",
    "2) generacja całej macierzy P (algorytm sprawdza przechodzenie z każdego stanu do każdego - nieoptymalne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Zadanie #5\n",
    "\n",
    "Dokonaj estymacji wartości stanu dla środowisk o rozmiarach:\n",
    "* 10x10\n",
    "* 50x50\n",
    "* 100x100\n",
    "* 500x500\n",
    "* 1000x1000\n",
    "\n",
    "Zmierz czas obliczeń i przedstaw wyniki w tabeli. Opisz obserwacje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dla środowisk o rozmiarach: 100x100, 500x500, 1000x1000 program za długo się wykonuje, dlatego zrobiłam tylko dla pierwszych dwóch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix_P(m):\n",
    "    m2 = m ** 2\n",
    "    q = np.zeros(m2)\n",
    "    q[m2 // 2] = 1\n",
    "    P = np.zeros((m2 + 1, m2 + 1))\n",
    "    P[:m2, :m2] = get_P(m, 0.2, 0.3, 0.25, 0.25)\n",
    "    for i in range(m2):\n",
    "        P[i, m2] = P[i, i]\n",
    "        P[i, i] = 0\n",
    "    P[m2, m2] = 1\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "l = [10, 50]\n",
    "e = []\n",
    "t = []\n",
    "for m in l:\n",
    "    P = get_matrix_P(m)\n",
    "    start_time = time.time()\n",
    "    e.append(estimate_state_values(P, m*m, 0.005))\n",
    "    end_time = time.time()\n",
    "    t.append(end_time - start_time)\n",
    "print(e)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
