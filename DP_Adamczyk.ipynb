{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Przykład - dostawa towarów do food-trucka\n",
    "\n",
    "Problem: ile każdego dnia właściciel food trucka powinien kupić kotletów do burgera, aby wypełnić zapotrzebowanie. Food truck działa jedynie od poniedziałku do piątku.\n",
    "- Każdego ranka właściciel podejmuje decyzję o kupnie $A=\\{0, 100, 200, 300, 400\\}$ burgerów. Koszt pojedynczego wynosi $c=4$.\n",
    "- Pojemność lodówki wynosi $C=400$ burgerów. Każdy burger niewykorzystany w piątek zostaje wyrzucony.\n",
    "- Każde zakupione burgery, które przekraczają aktualną pojemność lodówki, zostają wyrzucone.\n",
    "- Codzienne zapotrzebowanie na burgery jest zmienną losową $D$ o następującym rozkładzie:\n",
    "![](img/D_prob_mass_fn.png)\n",
    "- Zarobek netto per burger wynosi $b=7$\n",
    "- Liczba sprzedanych dziennie burgerów wynosi $min(zapotrzebowanie, dostępne-burgery)$\n",
    "\n",
    "Cel: maksymalizacja tygodniowych zarobków $(b-c)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Definicja środowiska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Środowisko rozszerza klasę Env z biblioteki OpenAI Gym\n",
    "class FoodTruck(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.v_demand = [100, 200, 300, 400]\n",
    "        self.p_demand = [0.3, 0.4, 0.2, 0.1]\n",
    "        self.capacity = self.v_demand[-1]\n",
    "        self.days = ['Mon', 'Tue', 'Wed',\n",
    "                     'Thu', 'Fri', \"Weekend\"]\n",
    "        self.unit_cost = 4\n",
    "        self.net_revenue = 7\n",
    "        self.action_space = [0, 100, 200, 300, 400]\n",
    "        # stan środowiska - (dzień tygodnia, zapas burgerów na początku dnia)\n",
    "        # stan środowiska = obserwacja agenta (środowisko w pełni obserwowalne)\n",
    "        self.state_space = [(\"Mon\", 0)] \\\n",
    "                           + [(d, i) for d in self.days[1:]\n",
    "                              for i in [0, 100, 200, 300]]\n",
    "\n",
    "    # metoda obliczająca następny stan środowiska oraz nagrodę\n",
    "    def get_next_state_reward(self, state, action, demand):\n",
    "        day, inventory = state\n",
    "        result = {}\n",
    "        result['next_day'] = self.days[self.days.index(day)\n",
    "                                       + 1]\n",
    "        result['starting_inventory'] = min(self.capacity, inventory + action)\n",
    "        result['cost'] = self.unit_cost * action\n",
    "        result['sales'] = min(result['starting_inventory'], demand)\n",
    "        result['revenue'] = self.net_revenue * result['sales']\n",
    "        result['next_inventory'] = result['starting_inventory'] - result['sales']\n",
    "        result['reward'] = result['revenue'] - result['cost']\n",
    "        return result\n",
    "\n",
    "    def get_transition_prob(self, state, action):\n",
    "        next_s_r_prob = {}\n",
    "        for ix, demand in enumerate(self.v_demand):\n",
    "            result = self.get_next_state_reward(state,\n",
    "                                                action,\n",
    "                                                demand)\n",
    "            next_s = (result['next_day'], result['next_inventory'])\n",
    "            reward = result['reward']\n",
    "            prob = self.p_demand[ix]\n",
    "            if (next_s, reward) not in next_s_r_prob:\n",
    "                next_s_r_prob[next_s, reward] = prob\n",
    "            else:\n",
    "                next_s_r_prob[next_s, reward] += prob\n",
    "        return next_s_r_prob\n",
    "\n",
    "    # metody potrzebne do symulacji środowiska\n",
    "    def reset(self):\n",
    "        self.day = \"Mon\"\n",
    "        self.inventory = 0\n",
    "        state = (self.day, self.inventory)\n",
    "        return state\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        day, inventory = state\n",
    "        if day == \"Weekend\":\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def step(self, action):\n",
    "        demand = np.random.choice(self.v_demand, p=self.p_demand)\n",
    "        result = self.get_next_state_reward((self.day, self.inventory),\n",
    "                                            action,\n",
    "                                            demand)\n",
    "        self.day = result['next_day']\n",
    "        self.inventory = result['next_inventory']\n",
    "        state = (self.day, self.inventory)\n",
    "        reward = result['reward']\n",
    "        done = self.is_terminal(state)\n",
    "        info = {'demand': demand, 'sales': result['sales']}\n",
    "        return state, reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Ewaluacja strategii (policy evaluation)\n",
    "\n",
    "Rozpatrzmy prostą strategię: na początku dnia, właściciel kupuje tyle burgerów, aby zapas był równy 200 lub 300 burgerów (każdą z opcji wybiera z równym prawdopodobieństwem ($0.5$)).\n",
    "Na przykład, jeżeli na początku dnia w lodówce jest 100 burgerów, zakupi on 100 lub 200 sztuk.\n",
    "\n",
    "Dokonajmy ewaluacji tej strategii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# metoda zwracająca strategię działania\n",
    "def base_policy(states):\n",
    "    policy = {}\n",
    "    for s in states:\n",
    "        day, inventory = s\n",
    "        prob_a = {}\n",
    "        if inventory >= 300:\n",
    "            prob_a[0] = 1\n",
    "        else:\n",
    "            prob_a[200 - inventory] = 0.5\n",
    "            prob_a[300 - inventory] = 0.5\n",
    "        policy[s] = prob_a\n",
    "    return policy  # dict: stan -> {akcja: prawdopodobieństwo}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# metoda zwracająca zaktualizowaną wartość dla stanu s\n",
    "def expected_update(env, v, s, prob_a, gamma):\n",
    "    expected_value = 0\n",
    "    for a in prob_a:\n",
    "        prob_next_s_r = env.get_transition_prob(s, a)\n",
    "        for next_s, r in prob_next_s_r:\n",
    "            expected_value += prob_a[a] * prob_next_s_r[next_s, r] * (r + gamma * v[next_s])\n",
    "    return expected_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# implementacja algorytmu 'ewaluacji strategii'\n",
    "def policy_evaluation(env, policy, max_iter=100, v = None, eps=0.1, gamma=1):\n",
    "    if not v:\n",
    "        v = {s: 0 for s in env.state_space}\n",
    "    k = 0\n",
    "    while True:\n",
    "        max_delta = 0\n",
    "        for s in v:\n",
    "            if not env.is_terminal(s):\n",
    "                v_old = v[s]\n",
    "                prob_a = policy[s]\n",
    "                v[s] = expected_update(env, v, s, prob_a, gamma)\n",
    "                max_delta = max(max_delta, abs(v[s] - v_old))\n",
    "        k += 1\n",
    "        if max_delta < eps:\n",
    "            print(\"Converged in\", k, \"iterations.\")\n",
    "            break\n",
    "        elif k == max_iter:\n",
    "            print(\"Terminating after\", k, \"iterations.\")\n",
    "            break\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 6 iterations.\n",
      "Expected weekly profit: 2515.0\n",
      "The state values: {('Mon', 0): 2515.0, ('Tue', 0): 1960.0, ('Tue', 100): 2360.0, ('Tue', 200): 2760.0, ('Tue', 300): 3205.0, ('Wed', 0): 1405.0, ('Wed', 100): 1805.0, ('Wed', 200): 2205.0, ('Wed', 300): 2650.0, ('Thu', 0): 850.0000000000001, ('Thu', 100): 1250.0, ('Thu', 200): 1650.0, ('Thu', 300): 2095.0, ('Fri', 0): 295.00000000000006, ('Fri', 100): 695.0000000000001, ('Fri', 200): 1095.0, ('Fri', 300): 1400.0, ('Weekend', 0): 0, ('Weekend', 100): 0, ('Weekend', 200): 0, ('Weekend', 300): 0}\n"
     ]
    }
   ],
   "source": [
    "foodtruck = FoodTruck()\n",
    "policy = base_policy(foodtruck.state_space)\n",
    "v = policy_evaluation(foodtruck, policy)\n",
    "print(\"Expected weekly profit:\", v[\"Mon\", 0])\n",
    "print(\"The state values:\", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Sprawdźmy, czy symulacja środowiska dla powyższej strategii da nam podobną wartość nagrody."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def choose_action(state, policy):\n",
    "    prob_a = policy[state]\n",
    "    action = np.random.choice(a=list(prob_a.keys()), p=list(prob_a.values()))\n",
    "    return action\n",
    "\n",
    "def simulate_policy(policy, n_episodes):\n",
    "    np.random.seed(0)\n",
    "    foodtruck = FoodTruck()\n",
    "    rewards = []\n",
    "    for i_episode in range(n_episodes):\n",
    "        state = foodtruck.reset()\n",
    "        done = False\n",
    "        ep_reward = 0\n",
    "        while not done:\n",
    "            action = choose_action(state, policy)\n",
    "            state, reward, done, info = foodtruck.step(action)\n",
    "            ep_reward += reward\n",
    "        rewards.append(ep_reward)\n",
    "    print(\"Expected weekly profit:\", np.mean(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected weekly profit: 2518.1\n"
     ]
    }
   ],
   "source": [
    "simulate_policy(policy, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Otrzymana wartość jest bliska wartości wyznaczonej analitycznie!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Iteracja strategii (policy iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def policy_improvement(env, v, s, actions, gamma):\n",
    "    prob_a = {}\n",
    "    if not env.is_terminal(s):\n",
    "        max_q = np.NINF\n",
    "        best_a = None\n",
    "        for a in actions:\n",
    "            q_sa = expected_update(env, v, s, {a: 1}, gamma)\n",
    "            if q_sa >= max_q:\n",
    "                max_q = q_sa\n",
    "                best_a = a\n",
    "        prob_a[best_a] = 1\n",
    "    else:\n",
    "        max_q = 0\n",
    "    return prob_a, max_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def policy_iteration(env,  eps=0.1, gamma=1):\n",
    "    np.random.seed(1)\n",
    "    states = env.state_space\n",
    "    actions = env.action_space\n",
    "    policy = {s: {np.random.choice(actions): 1} for s in states}\n",
    "    v = {s: 0 for s in states}\n",
    "    while True:\n",
    "        v = policy_evaluation(env, policy, v=v, eps=eps, gamma=gamma)\n",
    "        old_policy = policy\n",
    "        policy = {}\n",
    "        for s in states:\n",
    "            policy[s], _ = policy_improvement(env, v, s, actions, gamma)\n",
    "        if old_policy == policy:\n",
    "            break\n",
    "    print(\"Optimal policy found!\")\n",
    "    return policy, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 6 iterations.\n",
      "Converged in 6 iterations.\n",
      "Converged in 5 iterations.\n",
      "Optimal policy found!\n",
      "Expected weekly profit: 2880.0\n",
      "{('Mon', 0): {400: 1}, ('Tue', 0): {400: 1}, ('Tue', 100): {300: 1}, ('Tue', 200): {200: 1}, ('Tue', 300): {100: 1}, ('Wed', 0): {400: 1}, ('Wed', 100): {300: 1}, ('Wed', 200): {200: 1}, ('Wed', 300): {100: 1}, ('Thu', 0): {300: 1}, ('Thu', 100): {200: 1}, ('Thu', 200): {100: 1}, ('Thu', 300): {0: 1}, ('Fri', 0): {200: 1}, ('Fri', 100): {100: 1}, ('Fri', 200): {0: 1}, ('Fri', 300): {0: 1}, ('Weekend', 0): {}, ('Weekend', 100): {}, ('Weekend', 200): {}, ('Weekend', 300): {}}\n"
     ]
    }
   ],
   "source": [
    "policy, v = policy_iteration(foodtruck)\n",
    "print(\"Expected weekly profit:\", v[\"Mon\", 0])\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Zadanie\n",
    "\n",
    "1. Dokończ implementację metody *iteracja strategii (policy iteration)*.\n",
    "2. Opisz, na czym polega metoda *iteracja wartości (value iteration)*. Czym różni się ona od metody iteracja strategii?\n",
    "3. Podaj dwa powody, dla których zastosowanie programowania dynamicznego w praktyce jest trudne lub nawet niemożliwe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Opisz, na czym polega metoda iteracja wartości (value iteration). Czym różni się ona od metody iteracja strategii? <br>\n",
    "<br>\n",
    "Metoda iteracji strategii (policy iteration) polega na ulepszeniu poprzedniej strategii aż dojdziemy do optymalnej strategii i optymalnej funkjci wartości. Dla skończonego MDP mamy skończoną liczbę strategii, więc proces musi zbiegać do optymalnej strategii i optymalnej funkcji wartości w skończonej liczbie iteracji. Co więcej każda ocena strategii jest rozpoczynana z funkcją wartości dla poprzedniej strategii. Co oznacza, że każda z jej iteracji wymaga ewaluacji strategii, która może trwać bardzo długo, gdyż wymaga wielokrotnego przeczesywania zbioru stanów. Jednak można skrócić etap oceny strategii bez utraty gwarancji zbieżności iteracji strategii, gdy ocena polityki jest zatrzymywana po jednym przejściu (jednej aktualizacji każdego stanu). Taki algorytm nazywany jest iteracją wartości (value iteration), czyli ta metoda łączy w sobie etapy oceny strategii i jej poprawy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Podaj dwa powody, dla których zastosowanie programowania dynamicznego w praktyce jest trudne lub nawet niemożliwe <br>\n",
    "<br>\n",
    "- \"Curse of dimensionality\". Liczba stanów często rośnie wykładniczo wraz z liczbą zmiennych stanu, co oznacza, że duże zbiory stanów stwarzają problemy. (Duża ilość pamięci i obliczeń) .Jednym ze sposób na złagodzenie tego problemu może być asynchroniczne programowanie dynamiczne. \n",
    "<br> <br>\n",
    "- Problem ze środowiskiem. W teorii mamy podane nasze środowisko. Jakie ono jest, jakie są prawdopodobieństwo przejścia do stanów. Jednak w rzeczywistości nie jest to takie proste. W praktyce te prawdopodobieństwa są bardzo trudne do obliczenia lub czasami nawet nie możliwe do policzenia, nie znamy ich."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
