{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Dostpno modelu rodowiska\n",
    "\n",
    "Metody oparte na **value iteration** czy **policy iteration** opieraj si na wykorzystywaniu prawdopodobiestw przej rodowiska, aby wyznaczy optymalne strategie.\n",
    "W praktyce bardzo rzadko mamy wiedz o wewntrznej dynamice rodowiska lub jest ona bardzo trudna do wyznaczenia.\n",
    "\n",
    "Du偶o prostszym zadaniem jest pr贸bkowanie epizod贸w ze rodowiska oraz u偶ycie ich do wyznaczenia strategii bliskich optymalnym. Rodzin metod, kt贸re opieraj si na tej idei, s metody Monte Carlo.\n",
    "\n",
    "\n",
    "## Monte Carlo - model-free method\n",
    "\n",
    "We can estimate the state values and action values in an MDP from random samples.\n",
    "Monte Carlo (MC) estimation is a general concept that refers to making estimations through repeated random sampling. In the context of RL, it refers to\n",
    "**a collection of methods that estimates state values and action values using sample trajectories of complete episodes**.\n",
    "Using random samples is essential because often environment dynamics:\n",
    "- Is too complex to deal with\n",
    "- It is not known in the first place\n",
    "\n",
    "Summing up:\n",
    "- MC methods learn directly from episodes of experience\n",
    "- MC is *model-free*: no knowledge of MDP transitions/rewards\n",
    "- MC learns from *complete episodes*\n",
    "- MC uses the simplest idea: value = mean return\n",
    "- Caveat: can only apply MC to episodic MDPs (all episodes must terminate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Monte Carlo prediction\n",
    "\n",
    "We need to be able to evaluate a given policy to be able to improve it. MC prediction suggests simply observing (many) sample trajectories, sequences of state-action-reward tuples, starting in $S$, to estimate expectation $v_\\pi(s) = E_{\\pi}[G_{t}|S_{t}=s]$.\n",
    "MV policy evaluation uses *empirical mean* return instead of *expected* return.\n",
    "\n",
    "#### Przykad\n",
    "\n",
    "Robot umieszczony w rodowisku typu *Grid World* otrzymuje nagrod +1 za ka偶dy ruch. Epizod koczy si, gdy robot wpadnie na cian. Strategia dziaania jest deterministyczna, lecz po wybraniu akcji, agent ma 70% szansy na poruszenie si w wybranym kierunku oraz po 10% szansy na ka偶dy z pozostaych kierunk贸w (dynamika rodowiska).\n",
    "\n",
    "Rysunek poni偶ej przedstawia deterministyczn strategi $\\pi$ oraz dwie przykadowe trajektorie $\\tau$ przejcia agenta przez rodowisko (start w (1,2), epizod koczy si po wpadniciu na cian)\n",
    "\n",
    "![](img/example_robot.png)\n",
    "\n",
    "Aby wyznaczy warto oczekiwan wartoci $V_{\\pi}$ potrzebujemy (zgodnie z [definicj](https://pl.wikipedia.org/wiki/Warto%C5%9B%C4%87_oczekiwana)): sumy nagr贸d z epizodu (*return*, $G_{\\tau}$) oraz prawdopodobiestwa wystpienia danego epizodu ($p_{\\pi}(\\tau)$)\n",
    "\n",
    "#### $\\tau_{1}$\n",
    "\n",
    "- Kolejne stany: $S_{0}=(1,2), S_{1}=(1,1), S_{2}=(0,1), S_{3}=zderzenie$\n",
    "- Kolejne akcje: $A_{0}=d贸, A_{1}=g贸ra, A_{2}=prawo$\n",
    "- Kolejne nagrody: $R_{1}=1, R_{2}=1, R_{3}=0$\n",
    "- $p_{\\pi}(\\tau_{1})=0.7*0.1*0.1=0.007$\n",
    "- $G_{\\tau_{1}}=1+\\gamma 1+\\gamma^{2}0$\n",
    "-\n",
    "#### $\\tau_{2}$\n",
    "\n",
    "- Kolejne stany: $S_{0}=(1,2), S_{1}=(2,2), S_{2}=(2,1), S_{3}=(3,1), S_{4}=(2,1), S_{5}=(2,0), S_{6}=zderzenie$\n",
    "- Kolejne akcje: $A_{0}=d贸, A_{1}=g贸ra, A_{2}=prawo$\n",
    "- Kolejne nagrody: $R_{1}=1, R_{2}=1, R_{3}=0$\n",
    "- $p_{\\pi}(\\tau_{2})=0.1*0.7*0.1*0.7*0.1*0.1=0.000049$\n",
    "- $G_{\\tau_{2}}=1+\\gamma1+\\gamma^{2}1+\\gamma^{3}1+\\gamma^{4}1+\\gamma^{5}0$\n",
    "\n",
    "\n",
    "Aby wyznaczy warto stanu $s$, musimy wyznaczy $v_{\\pi}=E_{\\pi}[G_{t}|S_{t}=s]=E_{\\pi}[G_{\\tau}|S_{\\tau}=s]=\\sum_{i}p_{\\pi}(\\tau_{i})*G_{\\tau_{i}}$\n",
    "Do powy偶szej formuy aplikujemy wszystkie epizody, kt贸re **rozpoczy si w stanie $s$**.\n",
    "\n",
    "Wyznaczenie jej jest bardzo trudne, a nawet niemo偶liwe z uwagi na potencjalnie nieskoczon liczb mo偶liwych epizod贸w. Dlatego zamiast wartoci teoretycznej $v_{\\pi}(s)$ bdziemy u偶ywa estymacji tej wartoci np.:\n",
    "$\\hat{v}_{\\pi}(1,2)=\\dfrac{G_{\\tau_{1}}+G_{\\tau_{2}}}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### rodowisko Food truck\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Basia\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# rodowisko rozszerza klas Env z biblioteki OpenAI Gym\n",
    "class FoodTruck(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.v_demand = [100, 200, 300, 400]\n",
    "        self.p_demand = [0.3, 0.4, 0.2, 0.1]\n",
    "        self.capacity = self.v_demand[-1]\n",
    "        self.days = ['Mon', 'Tue', 'Wed',\n",
    "                     'Thu', 'Fri', \"Weekend\"]\n",
    "        self.unit_cost = 4\n",
    "        self.net_revenue = 7\n",
    "        self.action_space = [0, 100, 200, 300, 400]\n",
    "        # stan rodowiska - (dzie tygodnia, zapas burger贸w na pocztku dnia)\n",
    "        # stan rodowiska = obserwacja agenta (rodowisko w peni obserwowalne)\n",
    "        self.state_space = [(\"Mon\", 0)] \\\n",
    "                           + [(d, i) for d in self.days[1:]\n",
    "                              for i in [0, 100, 200, 300]]\n",
    "\n",
    "    # metoda obliczajca nastpny stan rodowiska oraz nagrod\n",
    "    def get_next_state_reward(self, state, action, demand):\n",
    "        day, inventory = state\n",
    "        result = {}\n",
    "        result['next_day'] = self.days[self.days.index(day)\n",
    "                                       + 1]\n",
    "        result['starting_inventory'] = min(self.capacity, inventory + action)\n",
    "        result['cost'] = self.unit_cost * action\n",
    "        result['sales'] = min(result['starting_inventory'], demand)\n",
    "        result['revenue'] = self.net_revenue * result['sales']\n",
    "        result['next_inventory'] = result['starting_inventory'] - result['sales']\n",
    "        result['reward'] = result['revenue'] - result['cost']\n",
    "        return result\n",
    "\n",
    "    def get_transition_prob(self, state, action):\n",
    "        next_s_r_prob = {}\n",
    "        for ix, demand in enumerate(self.v_demand):\n",
    "            result = self.get_next_state_reward(state,\n",
    "                                                action,\n",
    "                                                demand)\n",
    "            next_s = (result['next_day'], result['next_inventory'])\n",
    "            reward = result['reward']\n",
    "            prob = self.p_demand[ix]\n",
    "            if (next_s, reward) not in next_s_r_prob:\n",
    "                next_s_r_prob[next_s, reward] = prob\n",
    "            else:\n",
    "                next_s_r_prob[next_s, reward] += prob\n",
    "        return next_s_r_prob\n",
    "\n",
    "    # metody potrzebne do symulacji rodowiska\n",
    "    def reset(self):\n",
    "        self.day = \"Mon\"\n",
    "        self.inventory = 0\n",
    "        state = (self.day, self.inventory)\n",
    "        return state\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        day, inventory = state\n",
    "        if day == \"Weekend\":\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def step(self, action):\n",
    "        demand = np.random.choice(self.v_demand, p=self.p_demand)\n",
    "        result = self.get_next_state_reward((self.day, self.inventory),\n",
    "                                            action,\n",
    "                                            demand)\n",
    "        self.day = result['next_day']\n",
    "        self.inventory = result['next_inventory']\n",
    "        state = (self.day, self.inventory)\n",
    "        reward = result['reward']\n",
    "        done = self.is_terminal(state)\n",
    "        info = {'demand': demand, 'sales': result['sales']}\n",
    "\n",
    "        return state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Wybranie akcji dla danej strategii\n",
    "def choose_action(state, policy):\n",
    "    prob_a = policy[state]\n",
    "    action = np.random.choice(a=list(prob_a.keys()), p=list(prob_a.values()))\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# metoda zwracajca przykadow strategi dziaania\n",
    "def base_policy(states):\n",
    "    policy = {}\n",
    "    for s in states:\n",
    "        day, inventory = s\n",
    "        prob_a = {}\n",
    "        if inventory >= 300:\n",
    "            prob_a[0] = 1\n",
    "        else:\n",
    "            prob_a[200 - inventory] = 0.5\n",
    "            prob_a[300 - inventory] = 0.5\n",
    "        policy[s] = prob_a\n",
    "    return policy  # dict: stan -> {akcja: prawdopodobiestwo}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### \"First-visit\" vs \"every-visit\" Monte Carlo\n",
    "\n",
    "Za贸偶my, 偶e chcemy estymowa warto stanu $v_{\\pi}(1,2)$. Na rysunku poni偶ej widzimy trzy trajektorie: $\\tau_{1}, \\tau_{2}, \\tau_{3}, \\tau_{4}$. 呕aden z nich nie rozpoczyna si w punkcie $(1,2)$, ale mimo to mo偶emy u偶y trajektorii $\\tau_{1}, \\tau_{2}, \\tau_{3}$ do estymacji.\n",
    "Zauwa偶my, 偶e $\\tau_{3}$ dwukrotnie przechodzi przez punkt $(1,2)$. Czy mo偶emy u偶y sumy nagr贸d od pierwszej wizyty (first-visit) w tym punkcie, czy od ka偶dej z wizyt (every-visit)?\n",
    "\n",
    "![](img/first_vs_every.png)\n",
    "\n",
    "Oba podejcia s poprawne. \"Every-visit\" jest prostszy w u偶yciu przez aproksymatory funkcji (jak sieci neuronowe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def first_visit_return(returns, trajectory, gamma):\n",
    "    \"\"\"\n",
    "\n",
    "    :param returns: dict, klucze s stanami a wartoci jest lista sum nagr贸d uzyskanych z r贸偶nych trajektorii\n",
    "    :param trajectory: lista krotek: (stan, akcja, nagroda)\n",
    "    :param gamma: wsp贸czynnik dyskontowania\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    G = 0\n",
    "    T = len(trajectory) - 1\n",
    "    for t, sar in enumerate(reversed(trajectory)):\n",
    "        s, a, r = sar\n",
    "        G = r + gamma * G\n",
    "        first_visit = True\n",
    "        for j in range(T - t):\n",
    "            if s == trajectory[j][0]:\n",
    "                first_visit = False\n",
    "        if first_visit:\n",
    "            if s in returns:\n",
    "                returns[s].append(G) # dodaje sum nagr贸d z trajektorii do listy\n",
    "            else:\n",
    "                returns[s] = [G]\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_trajectory(env, policy):\n",
    "    trajectory = []\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    sar = [state]\n",
    "    while not done:\n",
    "        action = choose_action(state, policy)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        sar.append(action)\n",
    "        sar.append(reward)\n",
    "        trajectory.append(sar)\n",
    "        sar = [state]\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def first_visit_mc(env, policy, gamma, n_trajectories):\n",
    "    np.random.seed(0)\n",
    "    returns = {}\n",
    "    v = {}\n",
    "    for i in range(n_trajectories):\n",
    "        trajectory = get_trajectory(env, policy)\n",
    "        returns = first_visit_return(returns,\n",
    "                                     trajectory,\n",
    "                                     gamma)\n",
    "    for s in env.state_space:\n",
    "        if s in returns:\n",
    "            v[s] = np.round(np.mean(returns[s]), 1)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "foodtruck = FoodTruck()\n",
    "policy = base_policy(foodtruck.state_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Mon', 0): 2515.9,\n",
       " ('Tue', 0): 1959.1,\n",
       " ('Tue', 100): 2362.2,\n",
       " ('Tue', 200): 2765.2,\n",
       " ('Wed', 0): 1411.3,\n",
       " ('Wed', 100): 1804.2,\n",
       " ('Wed', 200): 2198.9,\n",
       " ('Thu', 0): 852.9,\n",
       " ('Thu', 100): 1265.4,\n",
       " ('Thu', 200): 1644.4,\n",
       " ('Fri', 0): 301.1,\n",
       " ('Fri', 100): 696.5,\n",
       " ('Fri', 200): 1097.2}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_est = first_visit_mc(foodtruck, policy, 1, 10000)\n",
    "v_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def every_visit_return(returns, trajectory, gamma):\n",
    "    G = 0\n",
    "    T = len(trajectory) - 1\n",
    "    for t, sar in enumerate(reversed(trajectory)):\n",
    "        s, a, r = sar\n",
    "        G = r + gamma * G\n",
    "        if s in returns:\n",
    "            returns[s].append(G) # dodaje sum nagr贸d z trajektorii do listy\n",
    "        else:\n",
    "            returns[s] = [G]\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def every_visit_mc(env, policy, gamma, n_trajectories):\n",
    "    np.random.seed(0)\n",
    "    returns = {}\n",
    "    v = {}\n",
    "    for i in range(n_trajectories):\n",
    "        trajectory = get_trajectory(env, policy)\n",
    "        returns = every_visit_return(returns,\n",
    "                                     trajectory,\n",
    "                                     gamma)\n",
    "    for s in env.state_space:\n",
    "        if s in returns:\n",
    "            v[s] = np.round(np.mean(returns[s]), 1)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Mon', 0): 2515.9,\n",
       " ('Tue', 0): 1959.1,\n",
       " ('Tue', 100): 2362.2,\n",
       " ('Tue', 200): 2765.2,\n",
       " ('Wed', 0): 1411.3,\n",
       " ('Wed', 100): 1804.2,\n",
       " ('Wed', 200): 2198.9,\n",
       " ('Thu', 0): 852.9,\n",
       " ('Thu', 100): 1265.4,\n",
       " ('Thu', 200): 1644.4,\n",
       " ('Fri', 0): 301.1,\n",
       " ('Fri', 100): 696.5,\n",
       " ('Fri', 200): 1097.2}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_est = every_visit_mc(foodtruck, policy, 1, 10000)\n",
    "v_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Widzimy, 偶e estymowane wartoci s bliskie wartociom rzeczywistym:\n",
    "\n",
    "![](img/v_pi_true_values.png)\n",
    "\n",
    "**Problem**, kt贸ry si pojawia, to brak wyznaczonych wartoci stan贸w dla stan贸w, gdzie agent na pocztku dnia wypenia zasoby o 300 jednostek. Dzieje si tak, dlatego, 偶e u偶yta strategia nie pozwala na odwiedzenie tych stan贸w!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Monte Carlo control\n",
    "\n",
    "Problem sterowania MC polega na znalezieniu optymalnej strategii wykorzystujc dowiadczenie agenta (trajektorie). Aby mie pewno, 偶e ka偶dy ze stan贸w zostanie odwiedzony, nale偶y zaimplementowa strategi **eksploracji**. W poni偶szym przykadzie wykorzystamy metod $\\epsilon-greedy$.\n",
    "\n",
    "##### $\\epsilon-greedy$\n",
    "\n",
    "Metoda ta wybiera losow akcj z prawdopodobiestwem $\\epsilon$ oraz akcj maksymalizujc funkcj akcja-warto z prawdopodobiestwem $1-\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Off-policy method\n",
    "Strategia wykorzystujca eksploracj nazywana jest **strategi zachowania (behavior policy), $b$**, natomiast strategia wykorzystujca jedynie najlepsze akcje (maksymalizacja funkcji akcja-warto) nazywana jest **strategi docelow (target policy), $\\pi$**.\n",
    "Powy偶sza metoda nazywana jest **off-policy**, poniewa偶 korzysta z obu wspomnianych strategii. Pierwsza z nich u偶ywana jest do generowania danych (trajektorii), natomiast druga jest docelow, optymaln strategi, kt贸rej szukamy.\n",
    "\n",
    "![](img/off_policy_mc_control.png)\n",
    "\n",
    "W metodach **on-policy** strategia zachowania jest u偶ywana do generacji danych oraz su偶y jako szukana strategia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Przykad zwracanej wartoci: {0: 0.96, 100: 0.01, 200: 0.01, 300: 0.01, 400: 0.01}\n",
    "# eps=0.05, najlepsza akcja=0\n",
    "def get_eps_greedy(actions, eps, a_best):\n",
    "    prob_a = {}\n",
    "    n_a = len(actions)\n",
    "    for a in actions:\n",
    "        if a == a_best:\n",
    "            prob_a[a] = 1 - eps + eps/n_a\n",
    "        else:\n",
    "            prob_a[a] = eps/n_a\n",
    "    return prob_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_random_policy(states, actions):\n",
    "    policy = {}\n",
    "    n_a = len(actions)\n",
    "    for s in states:\n",
    "        policy[s] = {a: 1/n_a for a in actions}\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def off_policy_mc(env, n_iter, eps, gamma):\n",
    "    np.random.seed(0)\n",
    "    states =  env.state_space\n",
    "    actions = env.action_space\n",
    "    Q = {s: {a: 0 for a in actions} for s in states}\n",
    "    C = {s: {a: 0 for a in actions} for s in states}\n",
    "    target_policy = {}\n",
    "    behavior_policy = get_random_policy(states,\n",
    "                                        actions)\n",
    "    for i in range(n_iter):\n",
    "        if i % 10000 == 0:\n",
    "            print(\"Iteration:\", i)\n",
    "        trajectory = get_trajectory(env, behavior_policy)\n",
    "        G = 0\n",
    "        W = 1\n",
    "        for t, sar in enumerate(reversed(trajectory)):\n",
    "            s, a, r = sar\n",
    "            G = r + gamma * G\n",
    "            C[s][a] += W\n",
    "            Q[s][a] += (W/C[s][a]) * (G - Q[s][a])\n",
    "            a_best = max(Q[s].items(), key=operator.itemgetter(1))[0]\n",
    "            target_policy[s] = a_best\n",
    "            behavior_policy[s] = get_eps_greedy(actions, eps, a_best)\n",
    "            if a != target_policy[s]:\n",
    "                break\n",
    "            W = W / behavior_policy[s][a]\n",
    "    target_policy = {s: target_policy[s] for s in states\n",
    "                     if s in target_policy}\n",
    "    return target_policy, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Iteration: 10000\n",
      "Iteration: 20000\n",
      "Iteration: 30000\n",
      "Iteration: 40000\n",
      "Iteration: 50000\n",
      "Iteration: 60000\n",
      "Iteration: 70000\n",
      "Iteration: 80000\n",
      "Iteration: 90000\n",
      "Iteration: 100000\n",
      "Iteration: 110000\n",
      "Iteration: 120000\n",
      "Iteration: 130000\n",
      "Iteration: 140000\n",
      "Iteration: 150000\n",
      "Iteration: 160000\n",
      "Iteration: 170000\n",
      "Iteration: 180000\n",
      "Iteration: 190000\n",
      "Iteration: 200000\n",
      "Iteration: 210000\n",
      "Iteration: 220000\n",
      "Iteration: 230000\n",
      "Iteration: 240000\n",
      "Iteration: 250000\n",
      "Iteration: 260000\n",
      "Iteration: 270000\n",
      "Iteration: 280000\n",
      "Iteration: 290000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('Mon', 0): 400,\n",
       " ('Tue', 0): 400,\n",
       " ('Tue', 100): 300,\n",
       " ('Tue', 200): 200,\n",
       " ('Tue', 300): 100,\n",
       " ('Wed', 0): 400,\n",
       " ('Wed', 100): 300,\n",
       " ('Wed', 200): 200,\n",
       " ('Wed', 300): 100,\n",
       " ('Thu', 0): 300,\n",
       " ('Thu', 100): 200,\n",
       " ('Thu', 200): 100,\n",
       " ('Thu', 300): 0,\n",
       " ('Fri', 0): 200,\n",
       " ('Fri', 100): 100,\n",
       " ('Fri', 200): 0,\n",
       " ('Fri', 300): 0}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy, Q = off_policy_mc(foodtruck, 300000, 0.05, 1)\n",
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Zadanie\n",
    "\n",
    "1. Co nale偶y zmieni w metodzie *first_visit_return*, aby zaimplementowa metod *every-visit*? Wynik $\\hat{v}$ powinien by identyczny z przypadkiem *first-visit*.\n",
    "2. Zaimplementuj metod $\\epsilon-greedy$.\n",
    "3. Opisz metod *importance sampling* wykorzystywan w metodach Monte Carlo. Jak rol odgrywa ona w powy偶szej metodzie *off-policy*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Co nale偶y zmieni w metodzie first_visit_return, aby zaimplementowa metod every-visit? Wynik  Ｌ powinien by identyczny z przypadkiem first-visit. <br>\n",
    "<br>\n",
    "Usunicie warunku (ptla for oraz if'y zwizane z first_visit). Zaimplementowaam metoda every_visit pod metod first_visit. Wyniki z obu metod s identyczne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Opisz metod importance sampling wykorzystywan w metodach Monte Carlo. Jak rol odgrywa ona w powy偶szej metodzie off-policy. <br>\n",
    "<br>\n",
    "Metoda importance sampling to og贸lna technika szacowania wartoci oczekiwanych w jednym rozkadzie na podstawie pr贸bek z innego rozkadu. Stosujemy t metod do uczenia si poza strategi (off-policy learning) poprzez wa偶enie zwrot贸w wedug wzgldnego prawdopodobiestwa wystpienia ich trajektorii w ramach strategii docelowej i strategii zachowania."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Materiay\n",
    "\n",
    "- Sutton, Barto, \"Reinforcement Learning an Introduction\", 2nd Edition, Chapter 5.\n",
    "- [Monte Carlo And Off-Policy Methods](https://www.youtube.com/watch?v=bpUszPiWM7o)\n",
    "- Mastering Reinforcement Learning with Python, p. 152-163"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
